# Workflow which performs end to end benchmarking of the agent process - it relies on starting up
# the agent and exercising various code with different config option.
name: Codespeed Agent Benchmark

on:
  workflow_call:
    inputs:
      job_name:
        description: "Github Action job name. Used to display user friendly job name in the UI."
        type: string
        required: true
      python-version:
        description: Python Version
        type: string
        required: true
      agent_config:
        description: "Path to the agent config file to use."
        type: string
        required: true
      codespeed_executable:
        description: "CodeSpeed executable name."
        type: string
        required: true
      codespeed_environment:
        description: "CodeSpeed environment name."
        type: string
        default: GitHub Hosted Action Runner
        required: false
      run_time:
        description: "How long to run the capture for (in seconds)."
        type: number
        default: 120
        required: false
      capture_interval:
        description: "How often to capture agent process level metrics during the process run time (in seconds)."
        type: number
        default: 5
        required: false
      agent_pre_run_command:
        description: "Optional bash command / script to run before starting the agent and the metrics capture script."
        type: string
        default: ""
        required: false
      agent_post_run_command:
        description: "Optional bash command / script to run after starting the agent and the metrics capture script."
        type: string
        default: ""
        required: false
      agent_server_host:
        description: "Value for the server_attributes.serverHost agent configuration option."
        type: string
        default: "ci-codespeed-benchmarks"
        required: false
      capture_agent_status_metrics:
        description: "True to capture additional metrics exposed via agent status command."
        type: boolean
        default: false
        required: false
      capture_line_counts:
        description: "True to submit log line counts for each log level to CodeSpeed."
        type: boolean
        default: false
        required: false
      dry_run:
        description: "True to enable dry run which runs the benchmarks without submitting data to CodeSpeed."
        type: boolean
        default: false
        required: false

jobs:
  codespeed-agent-benchmark:
    name: ${{ inputs.job_name }}
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        id: setup-python
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install Deps
        run: |
          python -m pip install --upgrade pip
          pip install -r benchmarks/scripts/requirements.txt

      - name: Capture commit date
        id: commit-date
        env:
          TZ: UTC
        run: |
          echo "commit-date=$(git show --quiet --date='format-local:%Y-%m-%d %H:%M:%S' --format='%cd' ${{ github.sha }})" >> $GITHUB_OUTPUT

      - name: Pre-Run Command
        if: inputs.agent_pre_run_command != ''
        run: ${{ inputs.agent_pre_run_command }}

      - name: Run Agent And Capture Resource Utilization
        id: run-agent
        env:
          CODESPEED_AUTH: ${{ secrets.CODESPEED_AUTH }}
          CODESPEED_URL: "https://scalyr-agent-codespeed.herokuapp.com/"
          CODESPEED_PROJECT: "scalyr-agent-2-procbenchmarks"
          CODESPEED_EXECUTABLE: ${{ inputs.codespeed_executable }}
          CODESPEED_ENVIRONMENT: ${{ inputs.codespeed_environment }}
          CODESPEED_BRANCH: ${{ github.head_ref || github.ref_name }}
          # NOTE: "idle" agent process (which monitors no logs but just runs the linux process
          # monitor for the agent process) should stabilize in a couple of minutes so it makes
          # no sense to run that benchmark longer.
          RUN_TIME: ${{ inputs.run_time }}
          CAPTURE_INTERVAL: ${{ inputs.capture_interval }}
          AGENT_CONFIG_FILE: ${{ inputs.agent_config }}
          SCALYR_SERVER: https://agent.scalyr.com
          SCALYR_API_KEY: ${{ secrets.CT_SCALYR_TOKEN_PROD_US_CLOUDTECH_TESTING_WRITE }}
          SCALYR_SERVER_ATTRIBUTES: "{\"serverHost\": \"${{ inputs.agent_server_host }}\"}"
          CAPTURE_AGENT_STATUS_METRICS: ${{ inputs.capture_agent_status_metrics }}
          DRY_RUN: ${{ inputs.dry_run }}
          COMMIT_DATE: ${{ steps.commit-date.outputs.commit-date }}
          PYTHONPATH: .
        run: |
          # Create directories which are needed by the agent process
          mkdir -p ~/scalyr-agent-dev/{log,config,data}
          # Run the agent process and capture the metrics
          ./benchmarks/scripts/start-agent-and-capture-metrics.sh "${{ github.sha }}" &
          bg_pid=$!
          echo "bg-pid=${bg_pid}" >> $GITHUB_OUTPUT

      - name: Run any post agent run script
        if: inputs.agent_post_run_command != ''
        run: |
          sleep 2
          ${{ inputs.agent_post_run_command }}

      - name: Wait for agent to finish
        timeout-minutes: 10
        run: tail --pid ${{ steps.run-agent.outputs.bg-pid }} -f /dev/null || true

      - name: Send line count values for various log levels
        if: inputs.capture_line_counts && ! inputs.dry_run
        env:
          CODESPEED_AUTH: ${{ secrets.CODESPEED_AUTH }}
          CODESPEED_URL: "https://scalyr-agent-codespeed.herokuapp.com/"
          CODESPEED_PROJECT: "scalyr-agent-2-procbenchmarks"
          CODESPEED_EXECUTABLE: ${{ inputs.codespeed_executable }}
          CODESPEED_ENVIRONMENT: ${{ inputs.codespeed_environment }}
          CODESPEED_BRANCH: ${{ github.head_ref || github.ref_name }}
          COMMIT_DATE: ${{ steps.commit-date.outputs.commit-date }}
          CAPTURE_AGENT_STATUS_METRICS: ${{ inputs.capture_agent_status_metrics }}
          DRY_RUN: ${{ inputs.dry_run }}
          PYTHONPATH: .
        run: |
          ./benchmarks/scripts/send-log-level-counts-to-codespeed.sh "${{ github.sha }}"

      - name: Upload log artifacts
        uses: actions/upload-artifact@v3
        with:
          name: agent-logs-${{ inputs.agent_server_host }}
          path: |
            ~/scalyr-agent-dev/log
        if: ${{ success() || failure() }}

      - name: Notify Slack on Failure
        if: ${{ failure() && github.ref_name == 'master' }}
        uses: act10ns/slack@ed1309ab9862e57e9e583e51c7889486b9a00b0f # v2.0.0
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          status: ${{ job.status }}
          steps: ${{ toJson(steps) }}
          channel: '#eng-dataset-cloud-tech'
